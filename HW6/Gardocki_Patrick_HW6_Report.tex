% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={HW6},
  pdfauthor={Patrick Gardocki},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{HW6}
\author{Patrick Gardocki}
\date{2023-11-29}

\begin{document}
\maketitle

\hypertarget{conceptual-questions}{%
\section{1. Conceptual Questions}\label{conceptual-questions}}

\hypertarget{a}{%
\subsection{a)}\label{a}}

Data-fit complexity is controlled in a regression tree by a method
called pruning. This method grows the tree to the max depth and then
removes nodes that do not contribute to the accuracy of the tree.

Limiting the max depth of the tree is one hyper parameter that can be
tuned to limit data over/under fitting. The max depth of a tree is
reached when each node contains a single value. For various tree depths,
observing where training accuracy starts to increase and validation
accuracy starts to decrease can help give an idea of the optimal tree
depth. Another hyperparamter that can be tuned is the minimum number of
samples required to be in a node. Adjusting this can simplify or
complicate the model.

\hypertarget{b}{%
\subsection{b)}\label{b}}

During training, each tree is trained on a bootstrapped sample. Not all
data points are in the sample. The data points left out are the out of
bag observations. Then each tree is evaluated on the left out data
points. The OOB error is calculated by comparing the predictions on the
OOB predictions to the actual outcomes. Observing when the OOB error
stabilizes, helps determine the optimal number of trees. Since the OOB
data points are within the training data set, and the OOB error is
utilized during training, it is a training error.

\hypertarget{c}{%
\subsection{c)}\label{c}}

Bagging trains several instances on different sample sets with
replacement (bootstrap samples). The results are then averaged.
Boosting, on the other hand, trains sequential weak learners and use
information from previous learners to improve the results. The result is
a weighted sum of the predictions, with better performers being weighed
more.

\hypertarget{d}{%
\subsection{d)}\label{d}}

There are various ways to limit over fit in CART models. This usually
occurs when the tree grows too deep and complex. Pruning is one method
to prevent over fit over fitting removing noncontributory parts of the
tree. Limiting the tree depth or leaf size also limit the tree from
growing too large and complex. Observing the validation set during
training can help determine when to stop the model training and prevent
over fitting.

\hypertarget{adaboost}{%
\section{2. AdaBoost}\label{adaboost}}

\hypertarget{a-1}{%
\subsection{a)}\label{a-1}}

Iteration 1: \(D_1(i)= \frac{1}{m}=\frac{1}{8}\) for all i.

\(h_1=sign(-x_1,-0.25)\)

\(\epsilon_1=\sum_{i=1}^{m}D_1(i)\mathbb{I}\{y^i\ne h_1(x^i)\}=\frac{1}{4}\)

\(\alpha_1 = \frac{1}{2}ln(\frac{1-\epsilon_1}{\epsilon_1})\approx 0.549\)

Iteration 2:
\(Z_1 = \sum_{i=1}^m D_1(i)e^{-\alpha_1 y^i h_1(x^i)} \approx 0.866\)

\(D_2(i) = \frac{D_1(i)}{Z_1}e^{-\alpha_1 y^i h_1(x^i)} \approx \left\{ \begin{array}{lcr} 0.083 & i \neq 5,6 \\ 0.250 & i =5,6\end{array}\right.\)

\(\epsilon_2 = \sum_{i=1}^m D_2(i) \mathbb{I}\{y^i \neq h_2(x^i)\} \approx 0.167\)

\(\alpha_2 = \frac 1 2 \ln(\frac{1-\epsilon_2}{\epsilon_2}) \approx 0.805\)

Iteration 3:
\(Z_2 = \sum_{i=1}^m D_2(i)e^{-\alpha_2 y^i h_2(x^i)} = \sum_{i\in\{1,2\}} D_2(i)e^{\alpha_2} + \sum_{i\notin \{1,2\}} D_2(i)e^{-\alpha_2} \approx 0.745\)

\(D_3(i) = \frac{D_2(i)}{Z_2}e^{-\alpha_2 y^i h_2(x^i)} \approx \left\{ \begin{array}{lcr} \approx 0.250 & i =1,2 \\ \approx 0.150 & i =5,6 \\ \approx 0.050 & i = 3,4,7,8 \end{array}\right.\)

\(h_3(x; w_3, b_3) = sign(h_3(x; -1, 0.75)) = sign( - x_2 + 0.75)\)

\(\epsilon_3 = \sum_{i=1}^m D_3(i) \mathbb{I}\{y^i \neq h_3 (x^i)\} = 0.100\)

\(\alpha_3 = \frac 1 2 \ln(\frac{1-\epsilon_3}{\epsilon_3}) \approx 1.099\)

\(Z_3 = \sum_{i=1}^m D_3(i)e^{-\alpha_3 y^i h_3(x^i)} = \sum_{i\in\{3,4\}} D_3(i)e^{\alpha_3} + \sum_{i\notin \{3,4\}} D_3(i)e^{-\alpha_3} \approx 0.60\)

\hypertarget{b-1}{%
\subsection{b)}\label{b-1}}

The training error is 0. each data point was correctly classified.
AdaBoost performs better than single decision stump because AdaBoost
lets future weak learners use weighted dataset from previous weak
learners, thus dividing higher dimension problems into several smaller
dimensional problems.

\begin{table}
\begin{center}
\caption{Values of AdaBoost parameters at each timestep.}
\vspace{0.1in}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
t & $\epsilon_t$ & $\alpha_t$ & $Z_t$ & $D_t(1)$ & $D_t(2)$ & $D_t(3)$ & $D_t(4)$ & $D_t(5)$ & $D_t(6)$ & $D_t(7)$ & $D_t(8)$ \\\hline
1 & 0.250 & 0.549 & 0.866 &0.125 &0.125 &0.125 &0.125 & 0.125&0.125 &0.125 & 0.125\\
2 & 0.167 & 0.805 & 0.745 & 0.083 & 0.083 & 0.083 & 0.083 & 0.250 & 0.250 & 0.083 & 0.083 \\
3 & 0.100 & 1.099 & 0.600 & 0.250 & 0.250 & 0.050 & 0.050 & 0.150 & 0.150 & 0.050 & 0.050 \\\hline
\end{tabular}
\end{center}
\end{table}

\hypertarget{random-forest-and-one-class-svm}{%
\section{3. Random Forest and one-class
SVM}\label{random-forest-and-one-class-svm}}

\hypertarget{a-2}{%
\subsection{a)}\label{a-2}}

\hypertarget{b-2}{%
\subsection{b)}\label{b-2}}

\hypertarget{c-1}{%
\subsection{c)}\label{c-1}}

\hypertarget{d-1}{%
\subsection{d)}\label{d-1}}

\hypertarget{locally-weighted-linear-regression}{%
\section{4. Locally weighted linear
regression}\label{locally-weighted-linear-regression}}

\hypertarget{a-3}{%
\subsection{a)}\label{a-3}}

\hypertarget{b-3}{%
\subsection{b)}\label{b-3}}

\hypertarget{c-2}{%
\subsection{c)}\label{c-2}}

\hypertarget{d-2}{%
\subsection{d)}\label{d-2}}

\hypertarget{e}{%
\subsection{e)}\label{e}}

\hypertarget{f}{%
\subsection{f)}\label{f}}

\end{document}
